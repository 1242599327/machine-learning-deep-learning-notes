# 深度学习基础

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [深度学习基础](#深度学习基础)
	- [为什么深度学习在近几年流行起来](#为什么深度学习在近几年流行起来)
	- [深度学习先驱：Jeff Hinton访谈](#深度学习先驱jeff-hinton访谈)
	- [神经网络的编程基础](#神经网络的编程基础)
		- [二分类问题](#二分类问题)
		- [符号定义](#符号定义)
	- [逻辑回归](#逻辑回归)
		- [向量化 Vectorization](#向量化-vectorization)
		- [向量化的更多例子](#向量化的更多例子)

<!-- /TOC -->

> 如果还不是很熟悉神经网络，请先去看机器学习里[神经网络](../machine-learning/neural-networks.md)的章节。

## 为什么深度学习在近几年流行起来

深度学习所使用的神经网络基础技术理念已经存在几十年了，为什么最近几年才流行起来呢？

通过一个图来了解一下。
图的水平轴是任务的数据量，垂直轴上是机器学习算法的性能。
比如准确率体现在垃圾邮件过滤或者广告点击预测，或者是神经网络在自动驾驶汽车时判断位置的准确性。如果把一个传统机器学习算法的性能画出来，作为数据量的一个函数，可能得到一个弯曲的线，就像图中红色的线。
它的性能一开始随着更多数据时会上升，但是一段变化后它的性能就会像一个天花板一样。假设你的水平轴拉的很长很长，它们不知道如何处理规模巨大的数据，而过去十年的社会里，我们遇到的很多问题只有相对较少的数据量。

<p align="center">
<img src="https://raw.github.com/fengdu78/deeplearning_ai_books/master/images/2b14edfcb21235115fca05879f8d9de2.png" />
</p>

随着数字化社会的发展，现在的数据量都非常巨大。比如在电脑网站上、在手机软件上以及其它数字化的服务，它们都能创建数据，同时便宜的相机被配置到移动电话，还有加速仪及各类各样的传感器，同时在物联网领域也收集到了越来越多的数据。仅仅在过去的20年里对于很多应用，便收集到了大量的数据，远超过机器学习算法能够高效发挥它们优势的规模。

神经网络展现出的是，如果训练一个小型的神经网络，那么这个性能可能会像图中黄色曲线表示那样；如果训练一个中等规模的神经网络（蓝色曲线），它在某些数据上面的性能会更好一些；如果训练一个非常大的神经网络，它就会变成下图绿色曲线那样，并且保持变得越来越好。

因此如果想要获得较高的性能体现，那么有两个条件要完成：
* 需要训练一个规模足够大的神经网络，以发挥数据规模量巨大的优点
* 需要能画到 _x_ 轴的这个位置，所以你需要很多的数据。

大家经常说规模一直在推动深度学习的进步，这里的"规模"同时指神经网络的规模。需要带有许多隐藏单元的神经网络，也有许多的参数及关联性，就如同需要大规模的数据一样。

事实上，如今在神经网络上获得更好性能的最可靠方法，是训练一个更大的神经网络，投入更多的数据。但这只能在一定程度上起作用，因为最终你耗尽了数据，或者最终你的网络是如此大规模导致将要用太久的时间去训练，但仅仅提升规模的的确确地让我们在深度学习的世界中摸索了很多时间。

当在一个小的训练集中，各种算法的优先级事实上定义的不是很明确，所以如果你没有大量的训练集，那效果会取决于你的特征工程能力，那将决定最终的性能。
假设有些人训练出了一个SVM（支持向量机）表现的更接近正确特征，然而有些人训练的规模大一些，可能在这个小的训练集中SVM算法可以做的更好。
因此知道在上图区域的左边，各种算法之间的优先级并不是定义的很明确，最终的性能更多的是取决于你在用工程选择特征方面的能力以及算法处理方面的一些细节，只是在某些大数据规模非常庞大的训练集，即在右边这个 _m_ 非常大时，我们能更加持续地看到更大的由神经网络控制的其它方法，因此如果你的任何某个朋友问你为什么神经网络这么流行，我会鼓励你也替他们画这样一个图形。

在深度学习技术发展的初期，数据规模以及计算量都局限了训练一个特别大的神经网络的能力。近年无论CPU还是GPU的发展，都使深度学习取得了巨大的进步。但是渐渐地，**尤其是在最近这几年，我们也见证了算法方面的极大创新**。许多算法方面的创新，一直是在尝试着使得神经网络运行的更快。

作为一个具体的例子，神经网络方面的一个巨大突破是从sigmoid函数转换到一个ReLU函数，这个函数我们在之前的课程里提到过。

<p align="center">
<img src="https://raw.github.com/fengdu78/deeplearning_ai_books/master/images/1a3d288dc243ca9c5a70a69799180c4a.png" />
</p>

使用Sigmoid函数的一个已知问题是，在函数的两端，Sigmoid函数的梯度会接近零，所以学习的速度会变得非常缓慢。
因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢。
因而可以改变激活函数，比如现在常用的ReLU的函数（修正线性单元）。
ReLU它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。
而这里的梯度，这条线的斜率在这左边是零，仅仅通过将Sigmod函数转换成ReLU函数，便能够使得梯度下降算法运行更快，这就是一个或许相对比较简单的算法创新的例子。

但是根本上算法创新所带来的影响，实际上是对计算带来的优化。有很多这样的例子，通过改变算法，使得代码运行的更快，这也使得我们能够训练规模更大的神经网络，或者是多端口的网络。
即使我们从所有的数据中拥有了大规模的神经网络，快速计算显得更加重要的另一个原因是，训练神经网络的过程，很多时候是凭借直觉的，往往你对神经网络架构有了一个想法，于是你尝试写代码实现你的想法，然后让你运行一个试验环境来告诉你，你的神经网络效果有多好，通过参考这个结果再返回去修改你的神经网络里面的一些细节，然后你不断的重复上面的操作。（如下右图）当神经网络需要很长时间去训练，需要很长时间重复这一循环，在这里就有很大的区别，根据你的生产效率去构建更高效的神经网络。当你能够有一个想法，试一试，看效果如何。
在10分钟内，或者也许要花上一整天，如果你训练你的神经网络用了一个月的时间，有时候发生这样的事情，也是值得的，因为你很快得到了一个结果。
在10分钟内或者一天内，你应该尝试更多的想法，那极有可能使得你的神经网络在你的应用方面工作的更好、更快的计算，在提高速度方面真的有帮助，那样你就能更快地得到你的实验结果。这也同时帮助了神经网络的实验人员和有关项目的研究人员在深度学习的工作中迭代的更快，也能够更快的改进你的想法，所有这些都使得整个深度学习的研究社群变的如此繁荣，包括令人难以置信地发明新的算法和取得不间断的进步，这些都是开拓者在做的事情，这些力量使得深度学习不断壮大。

<p align="center">
<img src="https://raw.github.com/fengdu78/deeplearning_ai_books/master/images/e26d18a882cfc48837118572dca51c56.png" />
</p>

这些力量目前不断的奏效，使得深度学习越来越好。研究表明社会仍然正在抛出越来越多的数字化数据，或者用一些特殊的硬件来进行计算，比如说GPU、TPU，以及更快的网络连接各种硬件。
非常有信心，未来可以实现一个超级大规模的神经网络，而计算的能力也会进一步的得到改善，还有算法相对的学习研究社区连续不断的在算法前沿产生非凡的创新。

## 深度学习先驱：Jeff Hinton访谈

<p align="center">
  <a href="https://www.youtube.com/watch?v=CId2ivZ6JZ8" target="_blank">
    <img src="https://img.youtube.com/vi/CId2ivZ6JZ8/0.jpg" />
  </a>
</p>


## 神经网络的编程基础
### 二分类问题
这一节将介绍神经网络的基础知识。在神经网络的计算中，通常先有一个前向传播(foward propagation)和一个反向传播(backward propagation)的步骤。

这里使用逻辑回归(logistic regression)来解释这些想法，以使你能够更加容易地理解这些概念。

逻辑回归是一个用于二分类(binary classification)的算法。首先我们从一个问题开始说起，这里有一个二分类问题的例子，假如你有一张图片作为输入，比如这只猫，如果识别这张图片为猫，则输出标签1作为结果；如果识别出不是猫，那么输出标签0作为结果。现在我们可以用字母 _y_ 来表示输出的结果标签，如下图所示：

<p align="center">
<img src="https://raw.github.com/fengdu78/deeplearning_ai_books/master/images/269118812ea785aee00f6ffc11b5c882.png" />
</p>

先看看一张图片在计算机中是如何表示的：
* 为保存一张图片，需要三个矩阵，分别对应图片中的红、绿、蓝三种颜色通道
* 如果图片大小为64x64像素，那么就有三个64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值

为了便于表示，这里画了三个很小的矩阵，注意它们的规模为5x4 而不是64x64，如下图所示：
<p align="center">
<img src="https://raw.github.com/fengdu78/deeplearning_ai_books/master/images/1e664a86fa2014d5212bcb88f1c419cf.png" />
</p>

为了把这些像素值放到一个特征向量中，需要把这些像素值提取出来，放入一个特征向量 _x_ 。
为了定义一个特征向量 _x_ 来表示这张图片。
首先把所有的像素都取出来，如果图片的大小为64x64像素，那么向量 _x_ 的总维度，将是64乘以64乘以3，这是三个像素矩阵中像素的总量。在这个例子中它为12,288。
现在我们用 _n<sub>x</sub>=12,288_ ，来表示输入特征向量的维度。
有时候为了简洁，会直接用小写的 _n_ 来表示输入特征向量 _x_ 的维度。

所以在二分类问题中，目标就是得到一个分类器，以图片的特征向量作为输入，然后预测输出结果 _y_ 为1还是0，也就是预测图片中是否有猫。

### 符号定义
这个系列课程里使用到的一些定义如下：
* _x_ ：表示一个 _n<sub>x</sub>_ 维数据，为输入数据，维度为 _(n<sub>x</sub>, 1)_ ；
* _y​_ ：表示输出结果，取值为 _(0, 1)​_ ；
* _(x<sup>(i)</sup>,y<sup>(i)</sup>)_ ：表示第 _i_ 组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据；
* _X=[x<sup>(1)</sup>, x<sup>(2)</sup>, ..., x<sup>(m)</sup>]_ ：表示所有的训练数据集的输入值，放在一个 _n<sub>x</sub> × m_ 的矩阵中，其中 _m_ 表示样本数目;
* _Y=[y<sup>(1)</sup>, y<sup>(2)</sup>, ..., y<sup>(m)</sup>]_ ：对应表示所有训练数据集的输出值，维度为 _1 × m_ 。
* 一对_(x, y)_ 来表示一个单独的样本， _x_ 代表 _n<sub>x</sub>_ 维的特征向量， _y_ 表示标签(输出结果)只能为0或1。

定义训练集由 _m_ 个训练样本组成时，有：
* _(x<sup>(1)</sup>,y<sup>(1)</sup>)_ 表示第一个样本的输入和输出
* _(x<sup>(m)</sup>,y<sup>(m)</sup>)_ 表示最后一个样本
* 有时为了强调这是训练样本的个数，会写作 _M<sub>train</sub>_
* 当涉及到测试集的时候，我们会使用 _M<sub>test</sub>_ 来表示测试集的样本数

定义一个矩阵用大写 _X_ 表示，它由输入向量 _x<sup>(1)</sup>_ 、 _x<sup>(2)</sup>_ 等组成。
如下图放在矩阵的列中，所以现在把 _x<sup>(1)</sup>_ 作为第一列放在矩阵中， _x<sup>(2)</sup>_ 作为第二列， _x<sup>(m)</sup>_ 放到第 _m_ 列，得到了训练集矩阵 _X_ 。
这个矩阵有 _m_ 列， _m_ 是训练集的样本数量，矩阵的高度记为 _n<sub>x</sub>_ 。

<p align="center">
<img src="https://raw.github.com/fengdu78/deeplearning_ai_books/master/images/55345ba411053da11ff843bbb3406369.png" />
</p>

一个好的符号定义能够将不同训练样本的数据很好地组织起来。
所说的数据不仅包括 _x_ 或者 _y_ 还包括之后你会看到的其他的量。
将不同的训练样本的数据提取出来，将他们堆叠在矩阵的列中，形成之后会在逻辑回归和神经网络上要用到的符号表示。
如果之后你忘了这些符号的意思，比如什么是 _m_ ，或者什么是 _n_ ，或者其他的，你可以随时查阅这里。

## 逻辑回归
这部分请参考机器学习中[逻辑回归](../machine-learning/logistic-regression.md)的章节。

### 向量化 Vectorization
向量化是非常基础的去除代码中for循环的艺术，在深度学习实践中，会经常发现训练大数据集，因为深度学习算法处理大数据集效果很棒，所以代码运行速度非常重要，否则如果在大数据集上，代码可能花费很长时间去运行，你将要等待非常长的时间去得到结果。

在深度学习领域，运行向量化是一个关键的技巧，举个例子说明什么是向量化。

在逻辑回归中你需要去计算 _z=w<sup>T</sup>x+b_ ， _w_ 、 _x_ 都是列向量。
如果你有很多的特征那么就会有一个非常大的向量，所以 _w ∈ R<sup>nx</sup>_ , _x ∈ R<sup>nx</sup>_ ，所以如果你想使用非向量化方法去计算 _w<sup>T</sup>x_ ，你需要用如下方式（Python）：
``` python
z = 0
for i in range(n_x):
    z = z + w[i]*x[i]
z = z + b
```

这是一个非向量化的实现，你会发现这真的很慢，作为一个对比，向量化实现将会非常直接计算 _w<sup>T</sup>x_ ，代码如下：

``` python
z = np.dot(w,x) + b
```

这是向量化计算 _w<sup>T</sup>x_ 的方法，你将会发现后一种写法运行速度相对会非常快。

让我们用一个小例子说明一下：
``` python
import numpy as np #导入numpy库
a = np.array([1,2,3,4]) #创建一个数据a
print(a) # [1 2 3 4]
import time
a = np.random.rand(1000000)
b = np.random.rand(1000000) #通过round随机得到两个一百万维度的数组

tic = time.time() #现在测量一下当前时间
""" 向量化的版本 """
c = np.dot(a,b)
toc = time.time()
print("Vectorized version:" + str(1000*(toc-tic)) +"ms") #打印一下向量化的版本的时间

""" 非向量化的版本 """
c = 0
tic = time.time()
for i in range(1000000):
    c += a[i]*b[i]
toc = time.time()
print(c)
print("For loop:" + str(1000*(toc-tic)) + "ms")#打印for循环的版本的时间
```

在两个方法中，向量化和非向量化计算了相同的值，如你所见，向量化版本花费了1.5毫秒，非向量化版本的for循环花费了大约几乎500毫秒，非向量化版本多花费了300倍时间。所以在这个例子中，仅仅是向量化你的代码，就会运行300倍快。这意味着如果向量化方法需要花费一分钟去运行的数据，for循环将会花费5个小时去运行。

一句话总结，以上都是再说和for循环相比，向量化可以快速得到结果。

你可能听过很多类似如下的话，“大规模的深度学习使用了GPU或者图像处理单元实现”，但是我做的所有的案例都是在jupyter notebook上面实现，这里只有CPU，CPU和GPU都有并行化的指令，他们有时候会叫做SIMD指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了built-in函数,像np.function或者并不要求你实现循环的函数，它可以让python的充分利用并行化计算，这是事实在GPU和CPU上面计算，GPU更加擅长SIMD计算，但是CPU事实上也不是太差，可能没有GPU那么擅长吧。接下来的视频中，你将看到向量化怎么能够加速你的代码，经验法则是，无论什么时候，避免使用明确的for循环。

### 向量化的更多例子

[回到顶部](#深度学习基础)
